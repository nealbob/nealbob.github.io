---
layout: post
title: Fast Python loops with Cython
excerpt: "My take on the basics of using Cython"
modified: 2014-10-30
tags: [Python, Cython, Machine Learning]
comments: true
---

[Cython](http://cython.org) is essentially a Python to C translator. Given Python like code ---  Python plus type declarations --- Cython generates C code. Cython provides a language almost as simple as Python, but with performance near that of C.  

# Should I use Cython?

If you are working in Python and have a genuine need for performance there are a number of options. First, you can try to exploit fast external packages: for example vectorizing your code via `numpy`. 

However, for many applications we can't escape the need for loops. One option is `numba` a just-in-time compiler for Python, another is give up on Python and learn `Julia` see [quantecon](http://quant-econ.net/python_or_julia.html).  

While the static compilation provided by Cython may not be cutting edge, Cython is mature, heavily used and well documented and its capable of handling large complicated projects. Cython code lies behind many of the big Python scientific libraries including `scikit-learn` and `pandas`.

# The example

Our example is evaluating a [Radial Basis Function network](http://en.wikipedia.org/wiki/Radial_basis_function_network) (RBF): a type of linear function approximation scheme. We assume each data point is a 'center' and use Gaussian type RBFs

<div>$$  \hat Y_i = \sum_{j=1}^N\beta_j exp(-\theta||X_i - X_j||) $$</div>

so we need a function which takes an input data array \\( X\\) of shape (N, D), a parameter array \\( \beta\\) of length N and a 'bandwidth' parameter \\(\theta\\) and returns an array of fitted values \\(  \hat Y \\) of length N. 

# Python loops

Here's the naive Python implementation

{% highlight python %}
def rbf_network(X, beta, theta):

    N = X.shape[0]
    D = X.shape[1]
    Y = np.zeros(N)

    for i in range(N):
        for j in range(N):
            r = 0
            for d in range(D):
                r += (X[j, d] - X[i, d]) ** 2
            Y[i] += beta[j] * exp(-(r * theta)**2)

    return Y
{% endhighlight %}
